{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('session/data_S1_Session1_S1_Session_1.h5', 'r') as f:\n",
    "    eeg = np.array(f['data'])\n",
    "    # So that the indexing works\n",
    "    labels = np.array(f['label']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_number_of_batches = eeg.shape[0] // batch_size\n",
    "max_no_data = max_number_of_batches * batch_size\n",
    "\n",
    "eeg_batches = eeg[:max_no_data]\n",
    "labels_batches = labels[:max_no_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_batches = eeg_batches.reshape(max_number_of_batches, batch_size, 1, 62, 500)\n",
    "labels_batches = labels_batches.reshape(max_number_of_batches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1102, 32, 1, 62, 500)\n",
      "(1102, 32)\n"
     ]
    }
   ],
   "source": [
    "print(eeg_batches.shape)\n",
    "print(labels_batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "index = rng.choice(max_number_of_batches, size=max_number_of_batches, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 800\n",
    "validation_size = 200\n",
    "\n",
    "# Test is not used\n",
    "\n",
    "train_eeg = eeg_batches[index[:train_size]]\n",
    "validation_eeg = eeg_batches[index[train_size:train_size+validation_size]]\n",
    "test_eeg = eeg_batches[index[train_size+validation_size:]]\n",
    "\n",
    "train_labels = labels_batches[index[:train_size]]\n",
    "validation_labels = labels_batches[index[train_size:train_size+validation_size]]\n",
    "test_labels = labels_batches[index[train_size+validation_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.data[idx]), torch.LongTensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_eeg, train_labels)\n",
    "validation_loader = DataLoader(validation_eeg, validation_labels)\n",
    "test_loader = DataLoader(test_eeg, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Convolutional Network block (Residual Block)\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        in_channels: number of channels\n",
    "        out_channels: number of output channels\n",
    "        kernel_size: the second parameter of the size of the kernel (because the first parameter needs to be one)\n",
    "        stride: stride\n",
    "        Padding - padding used for conv2d (dependant on the dilation)\n",
    "        Dilation - parameter used for conv2d, it determines when we end adding new blocks to our algorithm\n",
    "        Dropout - how many neurons should we drop\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1. Padding on the left side\n",
    "        pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "        # 2. Convolutional network (models usually include weight_norm)\n",
    "        conv2d1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, kernel_size), stride=stride, padding=0, dilation=dilation)\n",
    "        # 3. ELU\n",
    "        elu = nn.ELU()\n",
    "        # 4. Dropout\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        # 5. Add one more layer (only conv2d is need because the rest doesn't contain weights)\n",
    "        conv2d2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(1, kernel_size), stride=stride, padding=0, dilation=dilation)\n",
    "        # 6. Put everything in one place\n",
    "        self.net = nn.Sequential(pad, conv2d1, elu, dropout,\n",
    "                                 pad, conv2d2, elu, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow ConvNet (don't mistake with residual ConvNet)\n",
    "class ShallowNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_features, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        dilation = 1\n",
    "        tcn1 = TCNBlock(in_channels=in_channels, out_channels=40, kernel_size=kernel_size, stride=1, padding=(kernel_size-1) * dilation, dilation=1, dropout=0.1)\n",
    "        spatial_filter_1 = nn.Conv2d(in_channels=40, out_channels=40, kernel_size=(62, 1))\n",
    "        batch_norm = nn.BatchNorm2d(num_features=40)\n",
    "        elu = nn.ELU()\n",
    "        avg2dpool1 = nn.AvgPool2d(kernel_size=(1, 56), stride=(1, 14))\n",
    "        flatten = nn.Flatten(start_dim=1)\n",
    "        linear1 = nn.Linear(in_features=1280, out_features=out_features)\n",
    "#         softmax = nn.Softmax(dim=1)\n",
    "        self.model = nn.Sequential(tcn1, spatial_filter_1, batch_norm, elu, avg2dpool1, flatten, linear1, softmax)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShallowNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): TCNBlock(\n",
       "      (net): Sequential(\n",
       "        (0): ZeroPad2d(padding=(25, 0, 0, 0), value=0.0)\n",
       "        (1): Conv2d(1, 40, kernel_size=(1, 26), stride=(1, 1))\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): ZeroPad2d(padding=(25, 0, 0, 0), value=0.0)\n",
       "        (5): Conv2d(40, 40, kernel_size=(1, 26), stride=(1, 1))\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(40, 40, kernel_size=(62, 1), stride=(1, 1))\n",
       "    (2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): AvgPool2d(kernel_size=(1, 56), stride=(1, 14), padding=0)\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    (6): Linear(in_features=1280, out_features=4, bias=True)\n",
       "    (7): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = ShallowNetwork(in_channels=1, out_features=4, kernel_size=26, dropout=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.net.1.weight: no of params: 1040\n",
      "model.0.net.1.bias: no of params: 40\n",
      "model.0.net.5.weight: no of params: 41600\n",
      "model.0.net.5.bias: no of params: 40\n",
      "model.1.weight: no of params: 99200\n",
      "model.1.bias: no of params: 40\n",
      "model.2.weight: no of params: 40\n",
      "model.2.bias: no of params: 40\n",
      "model.6.weight: no of params: 5120\n",
      "model.6.bias: no of params: 4\n",
      "\n",
      "Overall amount of parameters: 147164\n"
     ]
    }
   ],
   "source": [
    "ov_no_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        ov_no_params += param.numel()\n",
    "        print(\"{name}: no of params: {no_param}\".format(name=name, no_param=param.numel()))\n",
    "        \n",
    "print(\"\\nOverall amount of parameters: \" + str(ov_no_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise NotImplementedError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are alive and we are at the epoch: 1 and at the point: 0 with time: 5.49254035949707\n",
      "We are alive and we are at the epoch: 1 and at the point: 20 with time: 78.39482307434082\n",
      "We are alive and we are at the epoch: 1 and at the point: 40 with time: 80.65927386283875\n",
      "We are alive and we are at the epoch: 1 and at the point: 60 with time: 76.90121555328369\n",
      "We are alive and we are at the epoch: 1 and at the point: 80 with time: 75.97425031661987\n",
      "We are alive and we are at the epoch: 1 and at the point: 100 with time: 74.57956004142761\n",
      "We are alive and we are at the epoch: 1 and at the point: 120 with time: 75.18793034553528\n",
      "We are alive and we are at the epoch: 1 and at the point: 140 with time: 73.59290337562561\n",
      "We are alive and we are at the epoch: 1 and at the point: 160 with time: 78.58671736717224\n",
      "We are alive and we are at the epoch: 1 and at the point: 180 with time: 78.45951199531555\n",
      "We are alive and we are at the epoch: 1 and at the point: 200 with time: 76.60233426094055\n",
      "We are alive and we are at the epoch: 1 and at the point: 220 with time: 78.16809844970703\n",
      "We are alive and we are at the epoch: 1 and at the point: 240 with time: 75.12449479103088\n",
      "We are alive and we are at the epoch: 1 and at the point: 260 with time: 80.70868253707886\n",
      "We are alive and we are at the epoch: 1 and at the point: 280 with time: 78.19157671928406\n",
      "We are alive and we are at the epoch: 1 and at the point: 300 with time: 83.86612725257874\n",
      "We are alive and we are at the epoch: 1 and at the point: 320 with time: 77.04944610595703\n",
      "We are alive and we are at the epoch: 1 and at the point: 340 with time: 85.49255180358887\n",
      "We are alive and we are at the epoch: 1 and at the point: 360 with time: 85.07185482978821\n",
      "We are alive and we are at the epoch: 1 and at the point: 380 with time: 83.06308937072754\n",
      "We are alive and we are at the epoch: 1 and at the point: 400 with time: 75.75441837310791\n",
      "We are alive and we are at the epoch: 1 and at the point: 420 with time: 70.39700365066528\n",
      "We are alive and we are at the epoch: 1 and at the point: 440 with time: 68.98384022712708\n",
      "We are alive and we are at the epoch: 1 and at the point: 460 with time: 70.23253560066223\n",
      "We are alive and we are at the epoch: 1 and at the point: 480 with time: 70.17536687850952\n",
      "We are alive and we are at the epoch: 1 and at the point: 500 with time: 69.35399055480957\n",
      "We are alive and we are at the epoch: 1 and at the point: 520 with time: 69.8226752281189\n",
      "We are alive and we are at the epoch: 1 and at the point: 540 with time: 68.88300657272339\n",
      "We are alive and we are at the epoch: 1 and at the point: 560 with time: 70.12398099899292\n",
      "We are alive and we are at the epoch: 1 and at the point: 580 with time: 73.49628329277039\n",
      "We are alive and we are at the epoch: 1 and at the point: 600 with time: 75.1265778541565\n",
      "We are alive and we are at the epoch: 1 and at the point: 620 with time: 70.7614483833313\n",
      "We are alive and we are at the epoch: 1 and at the point: 640 with time: 70.04458498954773\n",
      "We are alive and we are at the epoch: 1 and at the point: 660 with time: 70.13447833061218\n",
      "We are alive and we are at the epoch: 1 and at the point: 680 with time: 71.3695056438446\n",
      "We are alive and we are at the epoch: 1 and at the point: 700 with time: 82.30558276176453\n",
      "We are alive and we are at the epoch: 1 and at the point: 720 with time: 76.24824738502502\n",
      "We are alive and we are at the epoch: 1 and at the point: 740 with time: 80.76984357833862\n",
      "We are alive and we are at the epoch: 1 and at the point: 760 with time: 78.72852277755737\n",
      "We are alive and we are at the epoch: 1 and at the point: 780 with time: 76.2701723575592\n",
      "Train loss on epoch: 1 : 1117.2215620279312\n",
      "We are alive and we are at the epoch: 2 and at the point: 0 with time: 80.86163902282715\n",
      "We are alive and we are at the epoch: 2 and at the point: 20 with time: 84.22694706916809\n",
      "We are alive and we are at the epoch: 2 and at the point: 40 with time: 81.33632564544678\n",
      "We are alive and we are at the epoch: 2 and at the point: 60 with time: 78.99618363380432\n",
      "We are alive and we are at the epoch: 2 and at the point: 80 with time: 82.75055050849915\n",
      "We are alive and we are at the epoch: 2 and at the point: 100 with time: 80.81495428085327\n",
      "We are alive and we are at the epoch: 2 and at the point: 120 with time: 70.96671605110168\n",
      "We are alive and we are at the epoch: 2 and at the point: 140 with time: 77.72203993797302\n",
      "We are alive and we are at the epoch: 2 and at the point: 160 with time: 80.52009344100952\n",
      "We are alive and we are at the epoch: 2 and at the point: 180 with time: 83.89721345901489\n",
      "We are alive and we are at the epoch: 2 and at the point: 200 with time: 84.84027767181396\n",
      "We are alive and we are at the epoch: 2 and at the point: 220 with time: 81.79940724372864\n",
      "We are alive and we are at the epoch: 2 and at the point: 240 with time: 79.78399300575256\n",
      "We are alive and we are at the epoch: 2 and at the point: 260 with time: 85.48729038238525\n",
      "We are alive and we are at the epoch: 2 and at the point: 280 with time: 79.35190057754517\n",
      "We are alive and we are at the epoch: 2 and at the point: 300 with time: 80.33514761924744\n",
      "We are alive and we are at the epoch: 2 and at the point: 320 with time: 77.36256527900696\n",
      "We are alive and we are at the epoch: 2 and at the point: 340 with time: 74.63712882995605\n",
      "We are alive and we are at the epoch: 2 and at the point: 360 with time: 74.22474074363708\n",
      "We are alive and we are at the epoch: 2 and at the point: 380 with time: 74.88022184371948\n",
      "We are alive and we are at the epoch: 2 and at the point: 400 with time: 78.53038048744202\n",
      "We are alive and we are at the epoch: 2 and at the point: 420 with time: 76.43190503120422\n",
      "We are alive and we are at the epoch: 2 and at the point: 440 with time: 75.02687811851501\n",
      "We are alive and we are at the epoch: 2 and at the point: 460 with time: 79.00086569786072\n",
      "We are alive and we are at the epoch: 2 and at the point: 480 with time: 80.97190499305725\n",
      "We are alive and we are at the epoch: 2 and at the point: 500 with time: 85.44050192832947\n",
      "We are alive and we are at the epoch: 2 and at the point: 520 with time: 84.45195722579956\n",
      "We are alive and we are at the epoch: 2 and at the point: 540 with time: 81.02076601982117\n",
      "We are alive and we are at the epoch: 2 and at the point: 560 with time: 82.05845141410828\n",
      "We are alive and we are at the epoch: 2 and at the point: 580 with time: 88.44580936431885\n",
      "We are alive and we are at the epoch: 2 and at the point: 600 with time: 108.83292555809021\n",
      "We are alive and we are at the epoch: 2 and at the point: 620 with time: 122.24995756149292\n",
      "We are alive and we are at the epoch: 2 and at the point: 640 with time: 114.6491756439209\n",
      "We are alive and we are at the epoch: 2 and at the point: 660 with time: 97.1331090927124\n",
      "We are alive and we are at the epoch: 2 and at the point: 680 with time: 94.41700029373169\n",
      "We are alive and we are at the epoch: 2 and at the point: 700 with time: 99.77335214614868\n",
      "We are alive and we are at the epoch: 2 and at the point: 720 with time: 100.30200529098511\n",
      "We are alive and we are at the epoch: 2 and at the point: 740 with time: 105.22187757492065\n",
      "We are alive and we are at the epoch: 2 and at the point: 760 with time: 97.54039025306702\n",
      "We are alive and we are at the epoch: 2 and at the point: 780 with time: 99.25707459449768\n",
      "Train loss on epoch: 2 : 1098.8079157471657\n",
      "We are alive and we are at the epoch: 3 and at the point: 0 with time: 93.02439904212952\n",
      "We are alive and we are at the epoch: 3 and at the point: 20 with time: 100.89699840545654\n",
      "We are alive and we are at the epoch: 3 and at the point: 40 with time: 93.7411777973175\n",
      "We are alive and we are at the epoch: 3 and at the point: 60 with time: 91.99138879776001\n",
      "We are alive and we are at the epoch: 3 and at the point: 80 with time: 91.36828660964966\n",
      "We are alive and we are at the epoch: 3 and at the point: 100 with time: 97.15627002716064\n",
      "We are alive and we are at the epoch: 3 and at the point: 120 with time: 92.34460735321045\n",
      "We are alive and we are at the epoch: 3 and at the point: 140 with time: 96.44363403320312\n",
      "We are alive and we are at the epoch: 3 and at the point: 160 with time: 95.53217053413391\n",
      "We are alive and we are at the epoch: 3 and at the point: 180 with time: 97.24325966835022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are alive and we are at the epoch: 3 and at the point: 200 with time: 104.35272312164307\n",
      "We are alive and we are at the epoch: 3 and at the point: 220 with time: 105.73270559310913\n",
      "We are alive and we are at the epoch: 3 and at the point: 240 with time: 107.23386359214783\n",
      "We are alive and we are at the epoch: 3 and at the point: 260 with time: 107.56246089935303\n",
      "We are alive and we are at the epoch: 3 and at the point: 280 with time: 114.0200743675232\n",
      "We are alive and we are at the epoch: 3 and at the point: 300 with time: 114.16831254959106\n",
      "We are alive and we are at the epoch: 3 and at the point: 320 with time: 120.9426658153534\n",
      "We are alive and we are at the epoch: 3 and at the point: 340 with time: 118.26021265983582\n",
      "We are alive and we are at the epoch: 3 and at the point: 360 with time: 121.17269468307495\n",
      "We are alive and we are at the epoch: 3 and at the point: 380 with time: 124.66216325759888\n",
      "We are alive and we are at the epoch: 3 and at the point: 400 with time: 126.02198958396912\n",
      "We are alive and we are at the epoch: 3 and at the point: 420 with time: 123.67167735099792\n",
      "We are alive and we are at the epoch: 3 and at the point: 440 with time: 125.82981443405151\n",
      "We are alive and we are at the epoch: 3 and at the point: 460 with time: 141.1208713054657\n",
      "We are alive and we are at the epoch: 3 and at the point: 480 with time: 140.98253870010376\n",
      "We are alive and we are at the epoch: 3 and at the point: 500 with time: 138.79686760902405\n",
      "We are alive and we are at the epoch: 3 and at the point: 520 with time: 140.38736295700073\n",
      "We are alive and we are at the epoch: 3 and at the point: 540 with time: 141.0602285861969\n",
      "We are alive and we are at the epoch: 3 and at the point: 560 with time: 145.4743549823761\n",
      "We are alive and we are at the epoch: 3 and at the point: 580 with time: 149.45993494987488\n",
      "We are alive and we are at the epoch: 3 and at the point: 600 with time: 154.68187284469604\n",
      "We are alive and we are at the epoch: 3 and at the point: 620 with time: 159.79711985588074\n",
      "We are alive and we are at the epoch: 3 and at the point: 640 with time: 170.12039017677307\n",
      "We are alive and we are at the epoch: 3 and at the point: 660 with time: 182.88079929351807\n",
      "We are alive and we are at the epoch: 3 and at the point: 680 with time: 179.83492803573608\n",
      "We are alive and we are at the epoch: 3 and at the point: 700 with time: 189.90005326271057\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "no_epochs = 5\n",
    "for epoch in range(no_epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Here normally you should use data loader but I don't want right now to define data loaders\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        eeg_inputs, eeg_labels = train_data\n",
    "        optimizer.zero_grad()\n",
    "        eeg_prediction = model(eeg_inputs)\n",
    "        loss = criterion(eeg_prediction, eeg_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            end_time = time.time()\n",
    "            print(\"We are alive and we are at the epoch: {epoch} and at the point: {i} with time: {time}\".format(epoch=epoch+1, i=i, time=end_time-start_time))\n",
    "            start_time = end_time\n",
    "        \n",
    "    print(\"Train loss on epoch: {epoch} : {train_loss}\".format(epoch=epoch+1, train_loss=train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should use validation in the training but we will use it here to make it easier\n",
    "overall_accuracy_validation = 0\n",
    "for i, val_data in enumerate(validation_loader):\n",
    "    eeg_inputs, eeg_labels = val_data\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eeg_prediction = torch.argmax(model(eeg_inputs))\n",
    "        \n",
    "    accuracy = (eeg_labels == eeg_prediction).sum().item() / batch_size\n",
    "    overall_accuracy_validation += accuracy\n",
    "    \n",
    "    print(\"Accuracy for batch no {i}: {accuracy}\".format(i=i, accuracy=accuracy))\n",
    "\n",
    "print(\"Overall accuracy: \", accuracy/len(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
